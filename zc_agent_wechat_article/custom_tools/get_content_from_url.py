import asyncio
import os
import re
import json
from urllib.parse import urljoin, urlparse
from playwright.async_api import async_playwright
import trafilatura
import aiohttp


def sanitize_filename(filename: str) -> str:
    return re.sub(r'[\\/*?:"<>|]', "_", filename).strip()


# æ ¹æ® Content-Type æ˜ å°„æ‰©å±•å
CONTENT_TYPE_MAP = {
    'image/jpeg': '.jpg',
    'image/jpg': '.jpg',
    'image/png': '.png',
    'image/gif': '.gif',
    'image/webp': '.webp',
    'image/bmp': '.bmp',
    'image/svg+xml': '.svg',
}


async def download_image(session, img_url, base_name):
    """ä¸‹è½½å›¾ç‰‡ï¼Œå¹¶æ ¹æ®å“åº”å¤´è‡ªåŠ¨è®¾ç½®æ­£ç¡®æ‰©å±•å"""
    try:
        async with session.get(img_url) as resp:
            content_type = resp.headers.get('content-type', '').lower()
            if not content_type.startswith('image/'):
                print(f"âš ï¸ éå›¾ç‰‡èµ„æºï¼ˆ{content_type}ï¼‰: {img_url}")
                return

            # ç¡®å®šæ‰©å±•å
            ext = CONTENT_TYPE_MAP.get(content_type, '')
            if not ext:
                # å°è¯•ä» URL æå–åŸå§‹æ‰©å±•åï¼ˆå¦‚ .htmlï¼‰
                parsed = urlparse(img_url)
                original_ext = os.path.splitext(parsed.path)[1]
                ext = original_ext if original_ext else '.jpg'

            save_path = f"{base_name}{ext}"

            # é˜²æ­¢è¦†ç›–
            counter = 1
            original_save_path = save_path
            while os.path.exists(save_path):
                name, ext_ = os.path.splitext(original_save_path)
                save_path = f"{name}_{counter}{ext_}"
                counter += 1

            content = await resp.read()
            with open(save_path, "wb") as f:
                f.write(content)
            print(f"âœ… å·²ä¿å­˜å›¾ç‰‡: {save_path}")

    except Exception as e:
        print(f"âŒ ä¸‹è½½å›¾ç‰‡å‡ºé”™: {e} - {img_url}")


async def extract_article_content(url: str, headless: bool = True):
    async with async_playwright() as p:
        # æ·»åŠ æ›´å¤šæµè§ˆå™¨é€‰é¡¹æ¥é¿å…è¢«æ£€æµ‹
        browser = await p.chromium.launch(
            headless=headless,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-features=IsolateOrigins,site-per-process'
            ]
        )
        
        # åˆ›å»ºæ–°çš„ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åæ£€æµ‹å¤´éƒ¨ä¿¡æ¯
        context = await browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080},
            locale='zh-CN',
            timezone_id='Asia/Shanghai',
            extra_http_headers={
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0'
            }
        )
        
        # æ·»åŠ åˆå§‹åŒ–è„šæœ¬ï¼Œéšè— webdriver å±æ€§
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5]
            });
            
            Object.defineProperty(navigator, 'languages', {
                get: () => ['zh-CN', 'zh', 'en']
            });
        """)
        
        page = await context.new_page()

        try:
            print(f"æ­£åœ¨åŠ è½½é¡µé¢: {url}")
            await page.goto(
                url,
                wait_until="networkidle",
                timeout=60000
            )
            
            # é¢å¤–ç­‰å¾…ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
            await page.wait_for_timeout(2000)

            html = await page.content()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ 403 é”™è¯¯é¡µé¢
            if "403 Forbidden" in html or "WAF" in html:
                print("âš ï¸ æ£€æµ‹åˆ° WAF é˜»æ­¢ï¼Œå°è¯•ç­‰å¾…æ›´é•¿æ—¶é—´...")
                await page.wait_for_timeout(3000)
                html = await page.content()
                
                # å¦‚æœä»ç„¶æ˜¯ 403ï¼Œå°è¯•é‡æ–°åŠ è½½
                if "403 Forbidden" in html or "WAF" in html:
                    print("âš ï¸ ä»ç„¶è¢«é˜»æ­¢ï¼Œå°è¯•åˆ·æ–°é¡µé¢...")
                    await page.reload(wait_until="networkidle", timeout=60000)
                    await page.wait_for_timeout(2000)
                    html = await page.content()

            meta_extract = trafilatura.extract(
                html,
                url=url,
                with_metadata=True,
                output_format="json",
                include_comments=False,
                include_tables=True,
                no_fallback=False
            )

            if not meta_extract:
                return "", "", []

            data = json.loads(meta_extract)
            title = data.get("title", "æœªå‘½åæ–‡ç« ")
            clean_title = sanitize_filename(title)

            html_extract = trafilatura.extract(
                html,
                url=url,
                output_format="html",
                include_comments=False,
                include_tables=True,
                no_fallback=False
            )

            img_urls = []
            if html_extract:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html_extract, "html.parser")
                for img in soup.find_all("img", src=True):
                    src = img["src"]
                    abs_url = urljoin(url, src)
                    if abs_url.startswith(("http://", "https://")):
                        img_urls.append(abs_url)

            return clean_title, data.get("text", ""), img_urls

        except Exception as e:
            print(f"âŒ æå–å¤±è´¥: {e}")
            return "", "", []
        finally:
            await context.close()
            await browser.close()


async def main():
    url = "https://blog.csdn.net/m0_48891301/article/details/157393845"

    title, content, img_urls = await extract_article_content(url, headless=True)

    if not content:
        print("âŒ æœªèƒ½æå–æ–‡ç« å†…å®¹")
        return

    print("âœ… æ–‡ç« å†…å®¹æå–æˆåŠŸï¼\n")
    print("=" * 50)
    print(content[:1000] + "..." if len(content) > 1000 else content)
    print("=" * 50)

    # ä¿å­˜æ­£æ–‡
    txt_path = f"{title}.txt"
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"\nğŸ“„ æ­£æ–‡å·²ä¿å­˜è‡³: {txt_path}")

    # ä¸‹è½½æ­£æ–‡ä¸­çš„å›¾ç‰‡ï¼ˆåŒ…æ‹¬ .html ç»“å°¾çš„ï¼‰
    if img_urls:
        print(f"\nğŸ–¼ï¸ å‘ç° {len(img_urls)} å¼ æ­£æ–‡å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")
        connector = aiohttp.TCPConnector(limit=10)
        timeout = aiohttp.ClientTimeout(total=30)
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            tasks = []
            for i, img_url in enumerate(img_urls, start=1):
                base_name = f"{title}_{i}"
                tasks.append(download_image(session, img_url, base_name))
            await asyncio.gather(*tasks)
    else:
        print("\nğŸ–¼ï¸ æ­£æ–‡ä¸­æœªå‘ç°å›¾ç‰‡")


if __name__ == "__main__":
    asyncio.run(main())